{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ezbm5bFVCQVN",
        "outputId": "142329f7-64e5-4960-bb15-0db21fe812d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)   # paksa ulang jika perlu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0QO9NHcBUjM"
      },
      "outputs": [],
      "source": [
        "!pip install pandas requests beautifulsoup4 pdfminer.six lxml > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlsVjkZffJRA"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import urllib\n",
        "from concurrent.futures import ThreadPoolExecutor, wait\n",
        "from datetime import date\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pdfminer.high_level import extract_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On0MDtd4CaYp"
      },
      "outputs": [],
      "source": [
        "def create_path(folder_name):\n",
        "    path = os.path.join(os.getcwd(), folder_name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "\n",
        "def open_page(link):\n",
        "    count = 0\n",
        "    while count < 3:\n",
        "        try:\n",
        "            return BeautifulSoup(requests.get(link).text, \"lxml\")\n",
        "        except:\n",
        "            count += 1\n",
        "            time.sleep(5)\n",
        "\n",
        "\n",
        "def get_detail(soup, keyword):\n",
        "    try:\n",
        "        text = (\n",
        "            soup.find(lambda tag: tag.name == \"td\" and keyword in tag.text)\n",
        "            .find_next()\n",
        "            .get_text()\n",
        "            .strip()\n",
        "        )\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "def get_pdf(url, path_pdf):\n",
        "    try:\n",
        "        file = urllib.request.urlopen(url)\n",
        "        file_name = os.path.basename(url)\n",
        "        file_content = file.read()\n",
        "        with open(f\"{path_pdf}/{file_name}\", \"wb\") as out_file:\n",
        "            out_file.write(file_content)\n",
        "        return io.BytesIO(file_content), file_name\n",
        "    except:\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"M a h ka m a h A g u n g R e p u blik In d o n esia\\n\", \"\")\n",
        "    text = text.replace(\"Disclaimer\\n\", \"\")\n",
        "    text = text.replace(\n",
        "        \"Kepaniteraan Mahkamah Agung Republik Indonesia berusaha untuk selalu mencantumkan informasi paling kini dan akurat sebagai bentuk komitmen Mahkamah Agung untuk pelayanan publik, transparansi dan akuntabilitas\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"pelaksanaan fungsi peradilan. Namun dalam hal-hal tertentu masih dimungkinkan terjadi permasalahan teknis terkait dengan akurasi dan keterkinian informasi yang kami sajikan, hal mana akan terus kami perbaiki dari waktu kewaktu.\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Dalam hal Anda menemukan inakurasi informasi yang termuat pada situs ini atau informasi yang seharusnya ada, namun belum tersedia, maka harap segera hubungi Kepaniteraan Mahkamah Agung RI melalui :\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    text = text.replace(\n",
        "        \"Email : kepaniteraan@mahkamahagung.go.id    Telp : 021-384 3348 (ext.318)\\n\",\n",
        "        \"\",\n",
        "    )\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_data(link, keyword_url, path_output, path_pdf, today):\n",
        "    soup = open_page(link)\n",
        "    table = soup.find(\"table\", {\"class\": \"table\"})\n",
        "    judul = table.find(\"h2\").text if table.find(\"h2\") else \"\"\n",
        "\n",
        "    nomor = get_detail(table, \"Nomor\")\n",
        "    tingkat_proses = get_detail(table, \"Tingkat Proses\")\n",
        "    klasifikasi = get_detail(table, \"Klasifikasi\")\n",
        "    kata_kunci = get_detail(table, \"Kata Kunci\")\n",
        "    tahun = get_detail(table, \"Tahun\")\n",
        "    tanggal_register = get_detail(table, \"Tanggal Register\")\n",
        "    lembaga_peradilan = get_detail(table, \"Lembaga Peradilan\")\n",
        "    jenis_lembaga_peradilan = get_detail(table, \"Jenis Lembaga Peradilan\")\n",
        "    hakim_ketua = get_detail(table, \"Hakim Ketua\")\n",
        "    hakim_anggota = get_detail(table, \"Hakim Anggota\")\n",
        "    panitera = get_detail(table, \"Panitera\")\n",
        "    amar = get_detail(table, \"Amar\")\n",
        "    amar_lainnya = get_detail(table, \"Amar Lainnya\")\n",
        "    catatan_amar = get_detail(table, \"Catatan Amar\")\n",
        "    tanggal_musyawarah = get_detail(table, \"Tanggal Musyawarah\")\n",
        "    tanggal_dibacakan = get_detail(table, \"Tanggal Dibacakan\")\n",
        "    kaidah = get_detail(table, \"Kaidah\")\n",
        "    status = get_detail(table, \"Status\")\n",
        "    abstrak = get_detail(table, \"Abstrak\")\n",
        "\n",
        "    try:\n",
        "        link_pdf = soup.find(\"a\", href=re.compile(r\"/pdf/\"))[\"href\"]\n",
        "        file_pdf, file_name_pdf = get_pdf(link_pdf, path_pdf)\n",
        "        text_pdf = extract_text(file_pdf)\n",
        "        text_pdf = clean_text(text_pdf)\n",
        "    except:\n",
        "        link_pdf = \"\"\n",
        "        text_pdf = \"\"\n",
        "        file_name_pdf = \"\"\n",
        "\n",
        "    data = [\n",
        "        judul,\n",
        "        nomor,\n",
        "        tingkat_proses,\n",
        "        klasifikasi,\n",
        "        kata_kunci,\n",
        "        tahun,\n",
        "        tanggal_register,\n",
        "        lembaga_peradilan,\n",
        "        jenis_lembaga_peradilan,\n",
        "        hakim_ketua,\n",
        "        hakim_anggota,\n",
        "        panitera,\n",
        "        amar,\n",
        "        amar_lainnya,\n",
        "        catatan_amar,\n",
        "        tanggal_musyawarah,\n",
        "        tanggal_dibacakan,\n",
        "        kaidah,\n",
        "        status,\n",
        "        abstrak,\n",
        "        link,\n",
        "        link_pdf,\n",
        "        file_name_pdf,\n",
        "        text_pdf,\n",
        "    ]\n",
        "    result = pd.DataFrame(\n",
        "        [data],\n",
        "        columns=[\n",
        "            \"judul\",\n",
        "            \"nomor\",\n",
        "            \"tingkat_proses\",\n",
        "            \"klasifikasi\",\n",
        "            \"kata_kunci\",\n",
        "            \"tahun\",\n",
        "            \"tanggal_register\",\n",
        "            \"lembaga_peradilan\",\n",
        "            \"jenis_lembaga_peradilan\",\n",
        "            \"hakim_ketua\",\n",
        "            \"hakim_anggota\",\n",
        "            \"panitera\",\n",
        "            \"amar\",\n",
        "            \"amar_lainnya\",\n",
        "            \"catatan_amar\",\n",
        "            \"tanggal_musyawarah\",\n",
        "            \"tanggal_dibacakan\",\n",
        "            \"kaidah\",\n",
        "            \"status\",\n",
        "            \"abstrak\",\n",
        "            \"link\",\n",
        "            \"link_pdf\",\n",
        "            \"file_name_pdf\",\n",
        "            \"text_pdf\",\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    keyword_url = keyword_url.replace(\"/\", \" \")\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        keyword_url = \"\"\n",
        "    destination = f\"{path_output}/putusan_ma_{keyword_url}_{today}\"\n",
        "    if not os.path.isfile(f\"{destination}.csv\"):\n",
        "        result.to_csv(f\"{destination}.csv\", header=True, index=False)\n",
        "    else:\n",
        "        result.to_csv(f\"{destination}.csv\", mode=\"a\", header=False, index=False)\n",
        "\n",
        "\n",
        "def run_process(keyword_url, page, sort_date, path_output, path_pdf, today, max_data, total_collected):\n",
        "    if keyword_url.startswith(\"https\"):\n",
        "        link = f\"{keyword_url}&page={page}\"\n",
        "    else:\n",
        "        link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword_url}&page={page}\"\n",
        "    if sort_date:\n",
        "        link += \"&obf=TANGGAL_PUTUS&obm=desc\"\n",
        "\n",
        "    soup = open_page(link)\n",
        "    links = soup.find_all(\"a\", {\"href\": re.compile(\"/direktori/putusan\")})\n",
        "\n",
        "    for link in links:\n",
        "        if total_collected[0] >= max_data:\n",
        "            return\n",
        "        extract_data(link[\"href\"], keyword_url, path_output, path_pdf, today)\n",
        "        total_collected[0] += 1\n",
        "\n",
        "\n",
        "def run_scraper(keyword=None, url=None, sort_date=True, download_pdf=True, max_data=20):\n",
        "    if not keyword and not url:\n",
        "        print(\"Please provide a keyword or URL\")\n",
        "        return\n",
        "\n",
        "    path_output = '/content/drive/MyDrive/PK/CSV/'\n",
        "    path_pdf = '/content/drive/MyDrive/PK/PDF/'\n",
        "    today = date.today().strftime(\"%Y-%m-%d\")\n",
        "    link = f\"https://putusan3.mahkamahagung.go.id/search.html?q={keyword}&page=1\"\n",
        "    if url:\n",
        "        link = url\n",
        "\n",
        "    soup = open_page(link)\n",
        "    last_page = int(soup.find_all(\"a\", {\"class\": \"page-link\"})[-1].get(\"data-ci-pagination-page\"))\n",
        "    print(f\"Scraping {max_data} data...\")\n",
        "\n",
        "    if url:\n",
        "        keyword_url = url\n",
        "    else:\n",
        "        keyword_url = keyword\n",
        "\n",
        "    total_collected = [0]\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        futures = []\n",
        "        for page in range(1, last_page + 1):\n",
        "            if total_collected[0] >= max_data:\n",
        "                break\n",
        "            futures.append(executor.submit(\n",
        "                run_process, keyword_url, page, sort_date, path_output, path_pdf, today, max_data, total_collected\n",
        "            ))\n",
        "        wait(futures)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_scraper(\n",
        "    url=\"https://putusan3.mahkamahagung.go.id/search.html?q=korupsi&jenis_doc=putusan&t_put=2024\",\n",
        "    max_data=37\n",
        ")\n"
      ],
      "metadata": {
        "id": "MvCY0hQRSbxB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}